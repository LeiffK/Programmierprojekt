export const topicHtml = `
<!DOCTYPE html>
<html lang="de">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Was sind Banditen und Algorithmen?</title>
  <style>
    body {
      background: #0e0e0e;
      color: #e5e5e5;
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      padding: 24px;
      line-height: 1.7;
    }
    .container {
      max-width: 1100px;
      margin: 0 auto;
    }
    header {
      text-align: center;
      margin-bottom: 48px;
    }
    h1 {
      font-size: 2.5rem;
      font-weight: 800;
      color: #4fc3f7;
      margin-bottom: 8px;
    }
    .subtitle {
      font-size: 1.2rem;
      color: #aaa;
    }
    section {
      background: #171717;
      border: 1px solid #2a2a2a;
      padding: 24px;
      border-radius: 16px;
      margin-bottom: 24px;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.4);
    }
    h2 {
      font-size: 1.75rem;
      font-weight: 700;
      margin-bottom: 16px;
      color: #f0f0f0;
    }
    p {
      margin-bottom: 16px;
      color: #d5d5d5;
    }
    strong {
      color: #fff;
      font-weight: 600;
    }
    .info-box {
      background: #1a1a1a;
      border: 1px solid #333;
      padding: 16px;
      border-radius: 12px;
      margin-top: 16px;
    }
    .info-box p {
      margin: 8px 0;
    }
    .info-box-title {
      font-weight: 600;
      color: #f0f0f0;
      margin-bottom: 8px;
    }
    a {
      color: #4fc3f7;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <h1>Lerne Banditen und ihre Algorithmen kennen</h1>
      <p class="subtitle">Eine EinfÃ¼hrung mit Beispielen aus YouTube-Thumbnails</p>
    </header>

    <section>
      <h2>âœ¨ Was ist ein Bandit?</h2>
      <p>Stell dir vor, du stehst in einer Spielhalle mit vielen Spielautomaten. Jeder Automat sieht etwas anders aus und du darfst immer nur an einem spielen. Dein Ziel: mÃ¶glichst viel gewinnen. Aber du weiÃŸt nicht, welcher Automat am meisten auszahlt. Vielleicht ist der rote Automat der beste, vielleicht der blaue, du musst es ausprobieren. Genau so funktioniert ein <strong>Multi-Armed Bandit</strong> (mehrarmiger Bandit). Jeder â€Arm" ist eine Option, die du ausprobieren kannst.</p>
      <p>In unserem Projekt nutzen wir dieses Prinzip, um <strong>YouTube Thumbnails</strong> zu testen. Stell dir vor, du hast mehrere Vorschaubilder fÃ¼r dein Video. Welches bringt die meisten Klicks? Welches sorgt dafÃ¼r, dass Zuschauer mÃ¶glichst lange dranbleiben? Wir mÃ¼ssen immer wieder entscheiden: Probieren wir ein neues Bild aus oder bleiben wir bei dem, was bisher am besten funktioniert hat? Genau dieses AbwÃ¤gen macht das Banditen-Problem spannend und praxisrelevant. Es ist quasi ein Modell fÃ¼r <strong>A/B-Testing</strong>.</p>
      <div class="info-box">
        <div class="info-box-title">WeiterfÃ¼hrende Links:</div>
        <p>ğŸ¥ <strong>Video:</strong> <a href="https://www.youtube.com/watch?v=QTZYOPiQTUc" target="_blank">EinfÃ¼hrung Multiâ€‘Armed Bandits</a></p>
        <p>ğŸ“– <strong>ErklÃ¤rung:</strong> <a href="https://doi.org/10.48550/arXiv.1904.10040" target="_blank">Arxiv Paper zu Multiâ€‘Armed Bandits</a></p>
      </div>
    </section>

    <section>
      <h2>â“ Was ist ein Bernoulli Bandit?</h2>
      <p>Der Bernoulli Bandit ist die einfachste Variante. Er kennt nur zwei Ergebnisse: Erfolg oder Misserfolg. Bei YouTube bedeutet das: Jemand klickt auf dein Thumbnail (1) oder er klickt nicht (0). Mehr Informationen gibt es hier nicht.</p>
      <p><strong>Warum nutzt man den?</strong> Er ist perfekt, wenn wir nur eine einfache Ja/Nein-Frage beantworten wollen: Klickt ein Zuschauer oder nicht? Gerade am Anfang reicht das oft aus, um einen Ãœberblick zu bekommen. AuÃŸerdem ist er die Grundlage vieler bekannter Algorithmen, weil er mathematisch leichter zu berechnen ist.</p>
      <div class="info-box">
        <div class="info-box-title">WeiterfÃ¼hrende Links:</div>
        <p>ğŸ¥ <strong>Video:</strong> <a href="https://studyflix.de/statistik/bernoulli-experiment-4205" target="_blank">Bernoulli Experiment erklÃ¤rt</a></p>
        <p>ğŸ“– <strong>ErklÃ¤rung:</strong> <a href="https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf" target="_blank">Stanford Tutorial (Kapitel zu Bernoulli)</a></p>
      </div>
    </section>

    <section>
      <h2>ğŸŒ Was ist ein Gaussian Bandit?</h2>
      <p>Der Gaussian Bandit geht einen Schritt weiter. Statt nur 0 oder 1 zu liefern, gibt er einen Zahlenwert zurÃ¼ck. In unserem Projekt ist das die Watchtime in Sekunden. Also: Nicht nur â€hat jemand geklickt?", sondern â€wie lange hat die Person geschaut?". Das ist realistischer, weil nicht jeder Klick gleich wertvoll ist. Ein Klick mit 2 Sekunden Watchtime bringt weniger als einer mit 5 Minuten Watchtime.</p>
      <p><strong>Warum nutzt man den?</strong> Weil er uns mehr Details liefert. Wir lernen nicht nur, ob etwas funktioniert, sondern auch, wie gut es funktioniert. Damit kÃ¶nnen wir Thumbnails auswÃ¤hlen, die nicht nur viele Klicks, sondern auch lange Zuschauerzeiten bringen. Das ist fÃ¼r YouTube besonders wichtig.</p>
      <div class="info-box">
        <div class="info-box-title">WeiterfÃ¼hrende Links:</div>
        <p>ğŸ¥ <strong>Video:</strong> <a href="https://www.youtube.com/watch?v=iDzaoEwd0N0" target="_blank">Normalverteilung / GauÃŸ-Verteilung erklÃ¤rt</a></p>
        <p>ğŸ“– <strong>ErklÃ¤rung:</strong> <a href="https://www.datacamp.com/de/tutorial/gaussian-distribution" target="_blank">Tutorial: EinfÃ¼hrung in die GauÃŸ-Verteilung</a></p>
      </div>
    </section>

    <section>
      <h2>ğŸ“Š Greedy Algorithmus</h2>
      <p>Der Greedy Algorithmus (â€gierig") ist sehr direkt: Er wÃ¤hlt immer die Option, die bisher am besten abgeschnitten hat. Wenn also Thumbnail A bisher die meisten Klicks gebracht hat, dann wird immer wieder A angezeigt. Das klingt erstmal clever, hat aber einen Haken: Was ist, wenn Thumbnail B zwar am Anfang schlechter war, aber langfristig viel besser wÃ¤re? Greedy wÃ¼rde das nie herausfinden.</p>
      <div class="info-box">
        <div class="info-box-title">WeiterfÃ¼hrende Links:</div>
        <p>ğŸ¥ <strong>Video:</strong> <a href="https://www.youtube.com/watch?v=fM6mwe4ZRz4" target="_blank">Greedy Algorithmus erklÃ¤rt</a></p>
        <p>ğŸ“– <strong>ErklÃ¤rung:</strong> <a href="https://link.springer.com/book/10.1007/978-3-8348-2339-7" target="_blank">Buchkapitel zu Greedy-Strategien</a></p>
      </div>
    </section>

    <section>
      <h2>ğŸ¯ Îµ-Greedy Algorithmus</h2>
      <p>Îµ-Greedy (gesprochen: â€Epsilon-Greedy") versucht, dieses Problem zu lÃ¶sen. Meistens wÃ¤hlt er die beste Option, aber in einem kleinen Prozentsatz der FÃ¤lle (zum Beispiel 10 %) probiert er etwas anderes aus. Dadurch wird sichergestellt, dass auch neue Thumbnails getestet werden und man nicht in einer mÃ¶glicherweise falschen Entscheidung hÃ¤ngenbleibt.</p>
      <div class="info-box">
        <div class="info-box-title">WeiterfÃ¼hrende Links:</div>
        <p>ğŸ¥ <strong>Video:</strong> <a href="https://www.youtube.com/watch?v=EjYEsbg95x0" target="_blank">Îµ-Greedy erklÃ¤rt</a></p>
        <p>ğŸ“– <strong>ErklÃ¤rung:</strong> <a href="https://www.geeksforgeeks.org/machine-learning/epsilon-greedy-algorithm-in-reinforcement-learning/" target="_blank">Artikel bei GeeksforGeeks</a></p>
      </div>
    </section>

    <section>
      <h2>ğŸ“ˆ Gradient Bandit</h2>
      <p>Der Gradient Bandit denkt ein bisschen anders. Er bewertet nicht einfach jede Option mit einem festen Wert, sondern er berechnet eine Art â€Beliebtheitswahrscheinlichkeit". Mit jeder neuen Erfahrung passt er diese Wahrscheinlichkeiten an. Je Ã¶fter ein Thumbnail Erfolg bringt, desto grÃ¶ÃŸer wird die Wahrscheinlichkeit, dass es ausgewÃ¤hlt wird. Der Vorteil: Der Algorithmus reagiert dynamisch und passt sich im Laufe der Zeit an VerÃ¤nderungen an, zum Beispiel wenn Zuschauer plÃ¶tzlich andere Vorlieben haben.</p>
      <div class="info-box">
        <div class="info-box-title">WeiterfÃ¼hrende Links:</div>
        <p>ğŸ¥ <strong>Video:</strong> <a href="https://www.youtube.com/watch?v=tZcXP4MkEP0" target="_blank">Gradient Bandit erklÃ¤rt</a></p>
        <p>ğŸ“– <strong>ErklÃ¤rung:</strong> <a href="https://www.cs.mcgill.ca/~dprecup/courses/Winter2023/Lectures/3-gradient-bandit-2023.pdf" target="_blank">Lecture Notes zu Gradient Bandit</a></p>
      </div>
    </section>

    <section>
      <h2>ğŸ” Upper Confidence Bound (UCB)</h2>
      <p>Der UCB-Algorithmus hat einen cleveren Trick: Er berÃ¼cksichtigt nicht nur, welche Option bisher am besten war, sondern auch, wie sicher diese Erkenntnis ist. Wenn ein Thumbnail nur sehr wenige Male getestet wurde, gibt er ihm einen â€Bonus", um es Ã¶fter auszuprobieren. So wird verhindert, dass potenziell gute Thumbnails nie eine faire Chance bekommen. UCB ist wie ein neugieriger Spieler, der nicht nur auf den aktuellen Gewinn schaut, sondern auch ausprobiert, ob er vielleicht etwas Ã¼bersehen hat.</p>
      <div class="info-box">
        <div class="info-box-title">WeiterfÃ¼hrende Links:</div>
        <p>ğŸ¥ <strong>Video:</strong> <a href="https://www.youtube.com/watch?v=FgmMK6RPU1c" target="_blank">UCB Algorithmus erklÃ¤rt</a></p>
        <p>ğŸ“– <strong>ErklÃ¤rung:</strong> <a href="https://medium.com/@numsmt2/reinforcement-learning-chapter-2-multi-armed-bandits-part-4-upper-confidence-bound-action-589213a8a722" target="_blank">Medium-Artikel zu UCB</a></p>
      </div>
    </section>

    <section>
      <h2>ğŸ² Thompson Sampling</h2>
      <p>Thompson Sampling arbeitet mit Wahrscheinlichkeiten. Man kann es sich so vorstellen: FÃ¼r jedes Thumbnail gibt es eine â€Wahrscheinlichkeitsverteilung", wie gut es sein kÃ¶nnte. Bei jeder Entscheidung wird quasi ein Los gezogen und das Thumbnail mit dem besten Los gewinnt. Dadurch wird von Natur aus eine gute Mischung aus Erkunden und Ausnutzen erreicht. Viele Studien zeigen, dass Thompson Sampling in der Praxis oft besonders effizient ist.</p>
      <div class="info-box">
        <div class="info-box-title">WeiterfÃ¼hrende Links:</div>
        <p>ğŸ¥ <strong>Video:</strong> <a href="https://www.youtube.com/watch?v=TdjOAfk7iVA" target="_blank">Thompson Sampling erklÃ¤rt</a></p>
        <p>ğŸ“– <strong>ErklÃ¤rung:</strong> <a href="https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf" target="_blank">Stanford Tutorial zu Thompson Sampling</a></p>
      </div>
    </section>

    <section>
      <h2>ğŸš€ Optimistic Initial Values (OIV)</h2>
      <p>Hier startet jede Option mit einem unrealistisch hohen Anfangswert. Dadurch denkt der Algorithmus am Anfang: â€Alle Optionen sind super!". Also probiert er sie auch wirklich alle einmal aus. Mit der Zeit stellt sich dann heraus, welche tatsÃ¤chlich gut sind und welche nicht. Vorteil: Kein Thumbnail wird vergessen, alle haben zu Beginn die gleiche Chance, sich zu beweisen.</p>
      <div class="info-box">
        <div class="info-box-title">WeiterfÃ¼hrende Links:</div>
        <p>ğŸ“– <strong>ErklÃ¤rung:</strong> <a href="https://medium.com/@farbluestar/reinforcement-learning-part-03-355be5c7cae4" target="_blank">Artikel zu Optimistic Initial Values</a></p>
      </div>
    </section>
  </div>
</body>
</html>
`;
